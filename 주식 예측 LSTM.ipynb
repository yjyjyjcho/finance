{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"주식 예측 LSTM.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1Wa-9siB0RAYIP0Ku8tdTMkKXMUlRwIj2","authorship_tag":"ABX9TyO/g7LViGnk3U8jqfC2w6+I"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"ovLsLZrcHK_4","executionInfo":{"status":"ok","timestamp":1605081104068,"user_tz":-540,"elapsed":1476,"user":{"displayName":"우렉마지노","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-O47QQyryHBGXIfhftqjtuEUb4byCYi_4CRFH7Q=s64","userId":"07446739532334297381"}},"outputId":"5f3be25c-14cc-4e4e-e7fc-3cb2c4feca59","colab":{"base_uri":"https://localhost:8080/"}},"source":["# !pip install finance-datareader\n","!pip install TA_Lib-0.4.19-cp39-cp39-win_amd64.whl"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[31mERROR: TA_Lib-0.4.19-cp39-cp39-win_amd64.whl is not a supported wheel on this platform.\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"z4Fv4XH2IV2i"},"source":["현대차 : 005380 코스피\n"]},{"cell_type":"markdown","metadata":{"id":"Zjs288eJJslL"},"source":["LSTM sudocode"]},{"cell_type":"code","metadata":{"id":"Hk8f8LRBGGzp"},"source":["from keras.layers import LSTM\n","\n","model() = Sequential()\n","model.add(Embedding(max_features, 32))\n","model.add(LSTM(32))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(optimizer='rmsprop',\n","              loss='binary_crossentropy',\n","              metrics=['acc'])\n","\n","history = model.fit(input_train, y_train,\n","                    epochs=10,\n","                    batch_size=128,\n","                    validation_split=0.2)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nvWoMdDyHSTZ","executionInfo":{"status":"ok","timestamp":1605080454444,"user_tz":-540,"elapsed":2728,"user":{"displayName":"우렉마지노","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-O47QQyryHBGXIfhftqjtuEUb4byCYi_4CRFH7Q=s64","userId":"07446739532334297381"}},"outputId":"9144a5d2-718d-45f5-a787-5547d3f95e3d","colab":{"base_uri":"https://localhost:8080/","height":428}},"source":["import FinanceDataReader as fdr\n","import pandas as pd\n","\n","pd.set_option('display.max_row', 500)\n","pd.set_option('display.max_columns', 100)\n","# fdr.__version__\n","\n","period = ['2017-11-11', '2020-11-1']\n"," \n","df = fdr.DataReader('005380', period[0], period[1]) #현대차 코스피\n","\n","df = df.rename({'Close':'adj_close'}, axis='columns')\n","df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Open</th>\n","      <th>High</th>\n","      <th>Low</th>\n","      <th>adj_close</th>\n","      <th>Volume</th>\n","      <th>Change</th>\n","    </tr>\n","    <tr>\n","      <th>Date</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2017-11-13</th>\n","      <td>156500</td>\n","      <td>161500</td>\n","      <td>156000</td>\n","      <td>160500</td>\n","      <td>664551</td>\n","      <td>0.038835</td>\n","    </tr>\n","    <tr>\n","      <th>2017-11-14</th>\n","      <td>160000</td>\n","      <td>160500</td>\n","      <td>157500</td>\n","      <td>160500</td>\n","      <td>493631</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2017-11-15</th>\n","      <td>161500</td>\n","      <td>163000</td>\n","      <td>160000</td>\n","      <td>161500</td>\n","      <td>568064</td>\n","      <td>0.006231</td>\n","    </tr>\n","    <tr>\n","      <th>2017-11-16</th>\n","      <td>162000</td>\n","      <td>162000</td>\n","      <td>158000</td>\n","      <td>159500</td>\n","      <td>392185</td>\n","      <td>-0.012384</td>\n","    </tr>\n","    <tr>\n","      <th>2017-11-17</th>\n","      <td>158000</td>\n","      <td>159000</td>\n","      <td>156000</td>\n","      <td>157000</td>\n","      <td>493488</td>\n","      <td>-0.015674</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2020-10-26</th>\n","      <td>167000</td>\n","      <td>174000</td>\n","      <td>164000</td>\n","      <td>171500</td>\n","      <td>3018359</td>\n","      <td>0.026946</td>\n","    </tr>\n","    <tr>\n","      <th>2020-10-27</th>\n","      <td>173000</td>\n","      <td>178000</td>\n","      <td>171000</td>\n","      <td>172500</td>\n","      <td>3317704</td>\n","      <td>0.005831</td>\n","    </tr>\n","    <tr>\n","      <th>2020-10-28</th>\n","      <td>172000</td>\n","      <td>174000</td>\n","      <td>169500</td>\n","      <td>173500</td>\n","      <td>1419988</td>\n","      <td>0.005797</td>\n","    </tr>\n","    <tr>\n","      <th>2020-10-29</th>\n","      <td>169000</td>\n","      <td>171000</td>\n","      <td>168000</td>\n","      <td>170000</td>\n","      <td>1484537</td>\n","      <td>-0.020173</td>\n","    </tr>\n","    <tr>\n","      <th>2020-10-30</th>\n","      <td>169000</td>\n","      <td>169500</td>\n","      <td>164500</td>\n","      <td>164500</td>\n","      <td>1452259</td>\n","      <td>-0.032353</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>729 rows × 6 columns</p>\n","</div>"],"text/plain":["              Open    High     Low  adj_close   Volume    Change\n","Date                                                            \n","2017-11-13  156500  161500  156000     160500   664551  0.038835\n","2017-11-14  160000  160500  157500     160500   493631  0.000000\n","2017-11-15  161500  163000  160000     161500   568064  0.006231\n","2017-11-16  162000  162000  158000     159500   392185 -0.012384\n","2017-11-17  158000  159000  156000     157000   493488 -0.015674\n","...            ...     ...     ...        ...      ...       ...\n","2020-10-26  167000  174000  164000     171500  3018359  0.026946\n","2020-10-27  173000  178000  171000     172500  3317704  0.005831\n","2020-10-28  172000  174000  169500     173500  1419988  0.005797\n","2020-10-29  169000  171000  168000     170000  1484537 -0.020173\n","2020-10-30  169000  169500  164500     164500  1452259 -0.032353\n","\n","[729 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"hculdchoMyUk"},"source":["주가 예측 LSTM"]},{"cell_type":"code","metadata":{"id":"tOJN-yITJxDS"},"source":["import pandas as pd \n","import pandas_datareader as pdr\n","import talib \n","import matplotlib.pyplot as plt\n","import numpy as np\n","import scipy.stats as stats\n","import math\n","from sklearn.preprocessing import MinMaxScaler\n","\n","\n","import numpy as np\n","import tensorflow as tf\n","import random as python_random\n","\n","seed_number = 7\n","np.random.seed(seed_number)\n","python_random.seed(seed_number)\n","tf.set_random_seed(seed_number) # v1\n","# tf.random.set_seed(seed_number) # v2\n","\n","\n","######## data Load\n","# df = pd.read_csv('../../data/ch08/intc.csv',\n","#                  index_col='Date',\n","#                  parse_dates=True)\n","# sox_df = pd.read_csv('../../data/ch08/sox_df.csv',\n","#                      index_col='Date',\n","#                      parse_dates=True)\n","# vix_df = pd.read_csv('../../data/ch08/vix_df.csv',\n","#                      index_col='Date',\n","#                      parse_dates=True)\n","# snp500_df = pd.read_csv('../../data/ch08/s&p500.csv',\n","#                         index_col='Date',\n","#                         parse_dates=True)\n","\n","######## data features\n","\n","df['next_price'] = df['Adj Close'].shift(-1)\n","df['next_rtn'] = df['Close'] / df['Open'] -1\n","df['log_return'] = np.log(1 + df['Adj Close'].pct_change())\n","df['CCI'] = talib.CCI(df['High'], df['Low'], df['Adj Close'], timeperiod=14)\n","\n","#1.RA : Standard deviation rolling average\n","# Moving Average\n","df['MA5'] = talib.SMA(df['Close'],timeperiod=5)\n","df['MA10'] = talib.SMA(df['Close'],timeperiod=10)\n","df['RASD5'] = talib.SMA(talib.STDDEV(df['Close'], timeperiod=5, nbdev=1),timeperiod=5)\n","df['RASD10'] = talib.SMA(talib.STDDEV(df['Close'], timeperiod=5, nbdev=1),timeperiod=10)\n","\n","#2.MACD : Moving Average Convergence/Divergence\n","macd, macdsignal, macdhist = talib.MACD(df['Close'], fastperiod=12, slowperiod=26, signalperiod=9)\n","df['MACD'] = macd \n","\n","# Momentum Indicators\n","#3.CCI : Commodity Channel Index\n","df['CCI'] = talib.CCI(df['High'], df['Low'], df['Close'], timeperiod=14)\n","# Volatility Indicators \n","\n","#4.ATR : Average True Range\n","df['ATR'] = talib.ATR(df['High'], df['Low'], df['Close'], timeperiod=14)\n","\n","#5.BOLL : Bollinger Band\n","upper, middle, lower = talib.BBANDS(df['Close'],timeperiod=20,nbdevup=2,nbdevdn=2,matype=0)\n","df['ub'] = upper\n","df['middle'] = middle\n","df['lb'] = lower\n","\n","#7.MTM1 \n","df['MTM1'] = talib.MOM(df['Close'], timeperiod=1)\n","\n","#7.MTM3\n","df['MTM3'] = talib.MOM(df['Close'], timeperiod=3)\n","\n","#8.ROC : Rate of change : ((price/prevPrice)-1)*100\n","df['ROC'] = talib.ROC(df['Close'], timeperiod=60)\n","\n","#9.WPR : william percent range (Williams' %R)\n","df['WPR'] = talib.WILLR(df['High'], df['Low'], df['Close'], timeperiod=14)\n","\n","\n","\n","snp500_df = snp500_df.loc[:,['Close']].copy()\n","snp500_df.rename(columns={'Close':'S&P500'},inplace=True)\n","sox_df = sox_df.loc[:,['Close']].copy()\n","sox_df.rename(columns={'Close':'SOX'},inplace=True)\n","vix_df = vix_df.loc[:,['Close']].copy()\n","vix_df.rename(columns={'Close':'VIX'},inplace=True)\n","\n","df = df.join(snp500_df,how='left')\n","df = df.join(sox_df,how='left')\n","df = df.join(vix_df,how='left')\n","\n","df.head()\n","\n","#  feature list\n","# feature_list = ['Adj Close', 'log_return', 'CCI','next_price']\n","# 볼린저 밴드와 MACD를 어떻게 활용해야할까? 음. 아님 그냥 그대로 사용하는 건가?\n","feature1_list = ['Open','High','Low','Adj Close','Volume','log_return']\n","feature2_list = ['RASD5','RASD10','ub','lb','CCI','ATR','MACD','MA5','MA10','MTM1','MTM3','ROC','WPR']\n","feature3_list = ['S&P500', 'SOX', 'VIX']\n","# feature4_list = ['next_price']\n","feature4_list = ['next_rtn']\n","\n","all_features = feature1_list + feature2_list + feature3_list + feature4_list\n","\n","phase_flag = '3'\n","\n","if phase_flag == '1' :\n","    train_from = '2010-01-04'\n","    train_to = '2012-01-01'\n","\n","    val_from = '2012-01-01'\n","    val_to = '2012-04-01'\n","\n","    test_from = '2012-04-01'\n","    test_to = '2012-07-01'\n","\n","elif phase_flag == '2' :\n","    train_from = '2012-07-01'\n","    train_to = '2014-07-01'\n","\n","    val_from = '2014-07-01'\n","    val_to = '2014-10-01'\n","\n","    test_from = '2014-10-01'\n","    test_to = '2015-01-01'\n","    \n","else : \n","    train_from = '2015-01-01'\n","    train_to = '2017-01-01'\n","\n","    val_from = '2017-01-01'\n","    val_to = '2017-04-01'\n","\n","    test_from = '2017-04-01'\n","    test_to = '2017-07-01'\n","\n","\n","# train / validation / testing\n","train_df  = df.loc[train_from:train_to,all_features].copy()\n","val_df = df.loc[val_from:val_to,all_features].copy()\n","test_df   = df.loc[test_from:test_to,all_features].copy()\n","\n","train_df.head()\n","\n","\n","def min_max_normal(tmp_df):\n","    eng_list = []\n","    sample_df = tmp_df.copy()\n","    for x in all_features:\n","        if x in feature4_list :\n","            continue\n","        series = sample_df[x].copy()\n","        values = series.values\n","        values = values.reshape((len(values), 1))\n","        # train the normalization\n","        scaler = MinMaxScaler(feature_range=(0, 1))\n","        scaler = scaler.fit(values)\n","#         print('columns : %s , Min: %f, Max: %f' % (x, scaler.data_min_, scaler.data_max_))\n","        # normalize the dataset and print\n","        normalized = scaler.transform(values)\n","        new_feature = '{}_normal'.format(x)\n","        eng_list.append(new_feature)\n","        sample_df[new_feature] = normalized\n","    return sample_df, eng_list\n","\n","train_sample_df, eng_list =  min_max_normal(train_df)\n","val_sample_df, eng_list =  min_max_normal(val_df)\n","test_sample_df, eng_list = min_max_normal(test_df)\n","\n","train_sample_df.head()\n","\n","######## LSTM Model 훈련데이터 구분하기\n","\n","num_step = 5\n","num_unit = 200\n","\n","def create_dateset_binary(data, feature_list, step, n):\n","    '''\n","    다음날 시종가 수익률 라벨링.\n","    '''\n","    train_xdata = np.array(data[feature_list[0:n]])\n","    \n","    # 가장 뒤 n step을 제외하기 위해. 왜냐하면 학습 input으로는 어차피 10개만 주려고 하니깐.\n","    m = np.arange(len(train_xdata) - step)\n","    #     np.random.shuffle(m)  # shufflee은 빼자.\n","    x, y = [], []\n","    for i in m:\n","        a = train_xdata[i:(i+step)]\n","        x.append(a)\n","    x_batch = np.reshape(np.array(x), (len(m), step, n))\n","    \n","    train_ydata = np.array(data[[feature_list[n]]])\n","    # n_step 이상부터 답을 사용할 수 있는거니깐. \n","    for i in m + step :\n","        next_rtn = train_ydata[i][0]\n","        if next_rtn > 0 :\n","            label = 1\n","        else :\n","            label = 0\n","        y.append(label)\n","    y_batch = np.reshape(np.array(y), (-1,1))\n","    return x_batch, y_batch\n","\n","\n","eng_list = eng_list + feature4_list\n","n_feature = len(eng_list)-1\n","# LSTM할때 사용했던 소스코드.\n","x_train, y_train = create_dateset_binary(train_sample_df[eng_list], eng_list, num_step, n_feature)\n","x_val, y_val = create_dateset_binary(val_sample_df[eng_list], eng_list, num_step, n_feature)\n","x_test, y_test = create_dateset_binary(test_sample_df[eng_list], eng_list, num_step, n_feature)\n","\n","\n","from tensorflow.keras.utils import to_categorical\n","\n","y_train = to_categorical(y_train, 2)\n","y_val = to_categorical(y_val, 2)\n","y_test = to_categorical(y_test, 2)\n","\n","print(pd.DataFrame(y_train).sum())\n","print(pd.DataFrame(y_val).sum())\n","print(pd.DataFrame(y_test).sum())\n","\n","\n","######## 모델을 수정해보자 keras로\n","\n","x_train.shape[1]\n","# 이것은 전체 데이터를 242 rolling , 10 window, 2개 feature를 본다는 것이다.\n","# 2개 feature를 10개 묶음으로 보는데, 1칸씩 미루면서 보니 242개 데이터를 본다는 것이다.\n","\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.layers import Input, Dense, LSTM\n","from tensorflow.keras.layers import Activation, BatchNormalization\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras import backend as K\n","from tensorflow.keras import regularizers\n","\n","# LSTM 모델을 생성한다.\n","K.clear_session()\n","input_layer = Input(batch_shape=(None, x_train.shape[1], x_train.shape[2]))\n","layer_lstm_1 = LSTM(num_unit, return_sequences = True, recurrent_regularizer = regularizers.l2(0.01))(input_layer)\n","layer_lstm_1 = BatchNormalization()(layer_lstm_1)\n","layer_lstm_2 = LSTM(num_unit, return_sequences = True, recurrent_regularizer = regularizers.l2(0.01))(layer_lstm_1)\n","layer_lstm_2 = Dropout(0.25)(layer_lstm_2)\n","layer_lstm_3 = LSTM(num_unit, return_sequences = True, recurrent_regularizer = regularizers.l2(0.01))(layer_lstm_2)\n","layer_lstm_3 = BatchNormalization()(layer_lstm_3)\n","layer_lstm_4 = LSTM(num_unit, return_sequences = True, recurrent_regularizer = regularizers.l2(0.01))(layer_lstm_3)\n","layer_lstm_4 = Dropout(0.25)(layer_lstm_4)\n","layer_lstm_5 = LSTM(num_unit , recurrent_regularizer = regularizers.l2(0.01))(layer_lstm_4)\n","layer_lstm_5 = BatchNormalization()(layer_lstm_5)\n","output_layer = Dense(2, activation='sigmoid')(layer_lstm_5)\n","\n","model = Model(input_layer, output_layer)\n","model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n","\n","print(model.summary())\n","\n","y_val.shape\n","\n","history = model.fit(x_train,y_train,epochs=20, batch_size=10, validation_data=(x_val, y_val))\n","\n","\n","def plot_history(history):\n","    plt.figure(figsize=(15, 5))\n","    ax = plt.subplot(1, 2, 1)\n","    plt.plot(history.history[\"loss\"])\n","    plt.title(\"Train loss\")\n","    ax = plt.subplot(1, 2, 2)\n","    plt.plot(history.history[\"val_loss\"])\n","    plt.title(\"Test loss\")\n","    plt.savefig('sample.png')\n","\n","\n","plot_history(history) # 3단계\n","\n","plot_history(history) # 3단계\n","\n","# model.save('model_functional_open_close_binary_phase3.h5')\n","\n","\n","\n","######## 예측\n","\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","\n","predicted = model.predict(x_test)\n","y_pred = np.argmax(predicted, axis=1)\n","Y_test = np.argmax(y_test, axis=1)\n","cm = confusion_matrix(Y_test, y_pred)\n","report = classification_report(Y_test, y_pred)\n","\n","\n","\n","######## ROC AUC 커브 만들기\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import roc_curve, auc\n","\n","# Plot a confusion matrix.\n","# cm is the confusion matrix, names are the names of the classes.\n","def plot_confusion_matrix(cm, names, title='Confusion matrix', cmap=plt.cm.Blues):\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(names))\n","    plt.xticks(tick_marks, names, rotation=45)\n","    plt.yticks(tick_marks, names)\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    \n","\n","# Plot an ROC. pred - the predictions, y - the expected output.\n","def plot_roc(pred,y):\n","    fpr, tpr, _ = roc_curve(y, pred)\n","    roc_auc = auc(fpr, tpr)\n","\n","    plt.figure()\n","    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n","    plt.plot([0, 1], [0, 1], 'k--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic (ROC)')\n","    plt.legend(loc=\"lower right\")\n","    plt.show()\n","\n","\n","# 3단계\n","plot_roc(y_pred,Y_test)\n","from sklearn.metrics import roc_auc_score\n","roc_score = roc_auc_score(Y_test,y_pred)\n","print('ROC AUC 값 : {0:.4f}'.format(roc_score))\n","\n","# 3단계\n","plot_roc(y_pred,Y_test)\n","from sklearn.metrics import roc_auc_score\n","roc_score = roc_auc_score(Y_test,y_pred)\n","print('ROC AUC 값 : {0:.4f}'.format(roc_score))\n","\n","\n","######## Drop&Batch\n","\n","y_pred # 3단계\n","\n","tn = cm[0][0]\n","fn = cm[1][0]\n","tp = cm[1][1]\n","fp = cm[0][1]\n","if tp == 0:\n","    tp = 1\n","if tn == 0:\n","    tn = 1\n","if fp == 0:\n","    fp = 1\n","if fn == 0:\n","    fn = 1\n","TPR = float(tp)/(float(tp)+float(fn))\n","FPR = float(fp)/(float(fp)+float(tn))\n","accuracy = round((float(tp) + float(tn))/(float(tp) +\n","                                          float(fp) + float(fn) + float(tn)), 3)\n","specitivity = round(float(tn)/(float(tn) + float(fp)), 3)\n","sensitivity = round(float(tp)/(float(tp) + float(fn)), 3)\n","mcc = round((float(tp)*float(tn) - float(fp)*float(fn))/math.sqrt(\n","    (float(tp)+float(fp))\n","    * (float(tp)+float(fn))\n","    * (float(tn)+float(fp))\n","    * (float(tn)+float(fn))\n","), 3)\n","\n","f_output = open('binary_lstm_open_close_phase3_dropout_batch_Normal_3단계 test.txt', 'a')\n","f_output.write('=======\\n')\n","f_output.write('{}epochs_{}batch\\n'.format(\n","    20, 10))\n","f_output.write('TN: {}\\n'.format(tn))\n","f_output.write('FN: {}\\n'.format(fn))\n","f_output.write('TP: {}\\n'.format(tp))\n","f_output.write('FP: {}\\n'.format(fp))\n","f_output.write('TPR: {}\\n'.format(TPR))\n","f_output.write('FPR: {}\\n'.format(FPR))\n","f_output.write('accuracy: {}\\n'.format(accuracy))\n","f_output.write('specitivity: {}\\n'.format(specitivity))\n","f_output.write(\"sensitivity : {}\\n\".format(sensitivity))\n","f_output.write(\"mcc : {}\\n\".format(mcc))\n","f_output.write(\"{}\".format(report))\n","f_output.write('=======\\n')\n","f_output.close()\n","\n","# 예측.\n","# y_hat = model.predict(x_test, batch_size = 1)\n","print(len(y_test))\n","print(len(y_pred))\n","\n","train_sample_df['Adj Close'].plot()\n","\n","test_sample_df['Adj Close'].plot()\n","\n","\n","# 3단계 \n","lstm_book_df = test_sample_df[['Adj Close','next_rtn']].copy()\n","# ### 이 문제에 있어서 Series와 DataFrame의 차이는 뭐지?\n","t1 = pd.DataFrame(data = y_pred,columns=['position'],index = lstm_book_df.index[5:])\n","lstm_book_df = lstm_book_df.join(t1,how='left')\n","lstm_book_df.fillna(0,inplace=True)\n","lstm_book_df['ret'] = lstm_book_df['Adj Close'].pct_change()\n","lstm_book_df['lstm_ret'] = lstm_book_df['next_rtn'] * lstm_book_df['position'].shift(1)\n","lstm_book_df['lstm_cumret'] = (lstm_book_df['lstm_ret'] + 1).cumprod()\n","lstm_book_df['bm_cumret'] = (lstm_book_df['ret'] + 1).cumprod()\n","\n","lstm_book_df[['lstm_cumret','bm_cumret']].plot()\n","\n","lstm_book_df[['lstm_cumret','bm_cumret']].plot()\n","\n","\n","######## backtesting\n","\n","historical_max = lstm_book_df['Adj Close'].cummax()\n","daily_drawdown = lstm_book_df['Adj Close'] / historical_max - 1.0\n","historical_dd = daily_drawdown.cummin()\n","historical_dd.plot()\n","\n","\n","\n","######## BM\n","\n","CAGR = lstm_book_df.loc[lstm_book_df.index[-1],'bm_cumret'] ** (252./len(lstm_book_df.index)) -1\n","Sharpe = np.mean(lstm_book_df['ret']) / np.std(lstm_book_df['ret']) * np.sqrt(252.)\n","VOL = np.std(lstm_book_df['ret']) * np.sqrt(252.)\n","MDD = historical_dd.min()\n","print('CAGR : ',round(CAGR*100,2),'%')\n","print('Sharpe : ',round(Sharpe,2))\n","print('VOL : ',round(VOL*100,2),'%')\n","print('MDD : ',round(-1*MDD*100,2),'%')\n","\n","CAGR = lstm_book_df.loc[lstm_book_df.index[-1],'lstm_cumret'] ** (252./len(lstm_book_df.index)) -1\n","Sharpe = np.mean(lstm_book_df['lstm_ret']) / np.std(lstm_book_df['lstm_ret']) * np.sqrt(252.)\n","VOL = np.std(lstm_book_df['lstm_ret']) * np.sqrt(252.)\n","MDD = historical_dd.min()\n","print('CAGR : ',round(CAGR*100,2),'%')\n","print('Sharpe : ',round(Sharpe,2))\n","print('VOL : ',round(VOL*100,2),'%')\n","print('MDD : ',round(-1*MDD*100,2),'%')\n","\n","CAGR = lstm_book_df.loc[lstm_book_df.index[-1],'lstm_cumret'] ** (252./len(lstm_book_df.index)) -1\n","Sharpe = np.mean(lstm_book_df['lstm_ret']) / np.std(lstm_book_df['lstm_ret']) * np.sqrt(252.)\n","VOL = np.std(lstm_book_df['lstm_ret']) * np.sqrt(252.)\n","MDD = historical_dd.min()\n","print('CAGR : ',round(CAGR*100,2),'%')\n","print('Sharpe : ',round(Sharpe,2))\n","print('VOL : ',round(VOL*100,2),'%')\n","print('MDD : ',round(-1*MDD*100,2),'%')\n","\n"],"execution_count":null,"outputs":[]}]}